{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OrnsteinUhlenbeckProcess():\n",
    "    def __init__(self, theta, mu=0., sigma=1., dt=1e-2, x0=None, size=1, sigma_min=0.2, n_steps_annealing=1000):\n",
    "        self.theta = theta\n",
    "        self.mu = mu\n",
    "        self.dt = dt\n",
    "        self.x0 = x0\n",
    "        self.size = size\n",
    "        self.n_steps = 0\n",
    "\n",
    "        if sigma_min is not None:\n",
    "            self.m = -float(sigma - sigma_min) / float(n_steps_annealing)\n",
    "            self.c = sigma\n",
    "            self.sigma_min = sigma_min\n",
    "        else:\n",
    "            self.m = 0.\n",
    "            self.c = sigma\n",
    "            self.sigma_min = sigma\n",
    "\n",
    "        self.reset_states()\n",
    "\n",
    "    def sample(self):\n",
    "        x = self.x_prev + self.theta * (self.mu - self.x_prev) * self.dt + self.current_sigma * np.sqrt(self.dt) * np.random.normal(size=self.size)\n",
    "        self.x_prev = x\n",
    "        self.n_steps += 1\n",
    "        return x\n",
    "\n",
    "    def reset_states(self):\n",
    "        self.x_prev = self.x0 if self.x0 is not None else np.zeros(self.size)\n",
    "    @property\n",
    "    def current_sigma(self):\n",
    "        sigma = max(self.sigma_min, self.m * float(self.n_steps) + self.c)\n",
    "        return sigma\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46a554fdd5404745b4b1da112ad0b55a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0\tLast reward: -5.41\tAverage reward: 9.23\n",
      "Episode 10\tLast reward: -211.41\tAverage reward: -35.79\n",
      "Episode 20\tLast reward: -4.48\tAverage reward: -41.11\n",
      "Episode 30\tLast reward: -11.96\tAverage reward: -27.07\n",
      "Episode 40\tLast reward: -15.51\tAverage reward: -21.81\n",
      "Episode 50\tLast reward: -106.10\tAverage reward: -42.84\n",
      "Episode 60\tLast reward: -12.90\tAverage reward: -42.02\n",
      "Episode 70\tLast reward: -9.83\tAverage reward: -29.26\n",
      "Episode 80\tLast reward: -14.81\tAverage reward: -24.83\n",
      "Episode 90\tLast reward: -19.86\tAverage reward: -22.45\n",
      "Episode 100\tLast reward: -10.06\tAverage reward: -19.29\n",
      "Episode 110\tLast reward: -6.37\tAverage reward: -14.84\n",
      "Episode 120\tLast reward: -28.05\tAverage reward: -16.10\n",
      "Episode 130\tLast reward: -114.16\tAverage reward: -38.07\n",
      "Episode 140\tLast reward: -151.70\tAverage reward: -78.19\n",
      "Episode 150\tLast reward: -86.71\tAverage reward: -91.40\n",
      "Episode 160\tLast reward: -14.40\tAverage reward: -79.04\n",
      "Episode 170\tLast reward: -19.17\tAverage reward: -52.62\n",
      "Episode 180\tLast reward: -55.30\tAverage reward: -49.51\n",
      "Episode 190\tLast reward: -74.32\tAverage reward: -60.41\n",
      "Episode 200\tLast reward: -76.44\tAverage reward: -64.07\n",
      "Episode 210\tLast reward: -150.84\tAverage reward: -90.41\n",
      "Episode 220\tLast reward: -128.07\tAverage reward: -108.84\n",
      "Episode 230\tLast reward: -29.53\tAverage reward: -91.51\n",
      "Episode 240\tLast reward: -12.45\tAverage reward: -58.78\n",
      "Episode 250\tLast reward: -46.47\tAverage reward: -48.79\n",
      "Episode 260\tLast reward: -37.74\tAverage reward: -43.89\n",
      "Episode 270\tLast reward: -53.04\tAverage reward: -42.50\n",
      "Episode 280\tLast reward: -61.94\tAverage reward: -51.59\n",
      "Episode 290\tLast reward: -23.14\tAverage reward: -43.63\n",
      "Episode 300\tLast reward: -8.68\tAverage reward: -29.72\n",
      "Episode 310\tLast reward: 89.78\tAverage reward: -6.20\n",
      "Episode 320\tLast reward: 89.44\tAverage reward: 20.29\n",
      "Episode 330\tLast reward: 78.57\tAverage reward: 48.00\n",
      "Episode 340\tLast reward: 75.77\tAverage reward: 61.13\n",
      "Episode 350\tLast reward: 85.42\tAverage reward: 70.99\n",
      "Episode 360\tLast reward: 92.05\tAverage reward: 79.74\n",
      "Episode 370\tLast reward: 93.09\tAverage reward: 81.85\n",
      "Episode 380\tLast reward: 89.64\tAverage reward: 86.10\n",
      "Episode 390\tLast reward: 66.66\tAverage reward: 81.65\n",
      "Episode 400\tLast reward: 43.51\tAverage reward: 69.41\n",
      "Episode 410\tLast reward: 74.43\tAverage reward: 64.31\n",
      "Episode 420\tLast reward: 67.20\tAverage reward: 68.50\n",
      "Episode 430\tLast reward: 79.92\tAverage reward: 72.51\n",
      "Episode 440\tLast reward: 65.33\tAverage reward: 72.83\n",
      "Episode 450\tLast reward: 74.56\tAverage reward: 73.77\n",
      "Episode 460\tLast reward: 81.46\tAverage reward: 72.73\n",
      "Episode 470\tLast reward: 82.78\tAverage reward: 76.51\n",
      "Episode 480\tLast reward: 81.15\tAverage reward: 77.29\n",
      "Episode 490\tLast reward: 56.10\tAverage reward: 74.61\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import gym\n",
    "import numpy as np\n",
    "from itertools import count\n",
    "from collections import namedtuple\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical, Normal\n",
    "\n",
    "# Cart Pole\n",
    "'''\n",
    "parser = argparse.ArgumentParser(description='PyTorch actor-critic example')\n",
    "parser.add_argument('--gamma', type=float, default=0.99, metavar='G',\n",
    "                    help='discount factor (default: 0.99)')\n",
    "parser.add_argument('--seed', type=int, default=543, metavar='N',\n",
    "                    help='random seed (default: 543)')\n",
    "parser.add_argument('--render', action='store_true',\n",
    "                    help='render the environment')\n",
    "parser.add_argument('--log-interval', type=int, default=10, metavar='N',\n",
    "                    help='interval between training status logs (default: 10)')\n",
    "args = parser.parse_args()\n",
    "'''\n",
    "\n",
    "env = gym.make('MountainCarContinuous-v0')\n",
    "env.seed(1)\n",
    "torch.manual_seed(1)\n",
    "\n",
    "\n",
    "SavedAction = namedtuple('SavedAction', ['log_prob', 'value'])\n",
    "\n",
    "\n",
    "class Policy(nn.Module):\n",
    "    \"\"\"\n",
    "    implements both actor and critic in one model\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(Policy, self).__init__()\n",
    "        self.affine1 = nn.Linear(2, 128)\n",
    "\n",
    "        # actor's layer\n",
    "        self.action_head = nn.Linear(128, 1)\n",
    "\n",
    "        # critic's layer\n",
    "        self.value_head = nn.Linear(128, 1)\n",
    "\n",
    "        # action & reward buffer\n",
    "        self.saved_actions = []\n",
    "        self.rewards = []\n",
    "        \n",
    "        self.noise_gen = OrnsteinUhlenbeckProcess(theta = 0.25 , sigma_min=1.0)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        forward of both actor and critic\n",
    "        \"\"\"\n",
    "        x = F.relu(self.affine1(x))\n",
    "\n",
    "        # actor: choses action to take from state s_t \n",
    "        # by returning probability of each action\n",
    "        action_mean = self.action_head(x)[0]\n",
    "\n",
    "        # critic: evaluates being in the state s_t\n",
    "        state_values = self.value_head(x)\n",
    "\n",
    "        # return values for both actor and critic as a tupel of 2 values:\n",
    "        # 1. a list with the probability of each action over the action space\n",
    "        # 2. the value from state s_t \n",
    "        return action_mean, state_values\n",
    "\n",
    "\n",
    "model = Policy()\n",
    "optimizer = optim.Adam(model.parameters(), lr=3e-2)\n",
    "eps = np.finfo(np.float32).eps.item()\n",
    "\n",
    "\n",
    "def select_action(state , exploration = True):\n",
    "    state = torch.from_numpy(state).float()\n",
    "    probs, state_value = model(state)\n",
    "\n",
    "    # create a categorical distribution over the list of probabilities of actions\n",
    "    if exploration:\n",
    "        probs = probs + torch.from_numpy(model.noise_gen.sample())\n",
    "        \n",
    "    m = Normal(probs , 0.2)\n",
    "\n",
    "    # and sample an action using the distribution\n",
    "    action = m.sample()\n",
    "    \n",
    "    \n",
    "        \n",
    "    # save to action buffer\n",
    "    model.saved_actions.append(SavedAction(m.log_prob(action), state_value))\n",
    "\n",
    "    # the action to take (left or right)\n",
    "    return action.item()\n",
    "\n",
    "\n",
    "def finish_episode():\n",
    "    \"\"\"\n",
    "    Training code. Calcultes actor and critic loss and performs backprop.\n",
    "    \"\"\"\n",
    "    R = 0\n",
    "    saved_actions = model.saved_actions\n",
    "    policy_losses = [] # list to save actor (policy) loss\n",
    "    value_losses = [] # list to save critic (value) loss\n",
    "    returns = [] # list to save the true values\n",
    "\n",
    "    # calculate the true value using rewards returned from the environment\n",
    "    for r in model.rewards[::-1]:\n",
    "        # calculate the discounted value\n",
    "        R = r + 0.99 * R\n",
    "        returns.insert(0, R)\n",
    "\n",
    "    returns = torch.tensor(returns)\n",
    "    returns = (returns - returns.mean()) / (returns.std() + eps)\n",
    "\n",
    "    for (log_prob, value), R in zip(saved_actions, returns):\n",
    "        advantage = R - value.item()\n",
    "\n",
    "        # calculate actor (policy) loss \n",
    "        policy_losses.append(-log_prob * advantage)\n",
    "\n",
    "        # calculate critic (value) loss using L1 smooth loss\n",
    "        value_losses.append(F.smooth_l1_loss(value, torch.tensor([R])))\n",
    "\n",
    "    # reset gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # sum up all the values of policy_losses and value_losses\n",
    "    loss = torch.stack(policy_losses).sum() + torch.stack(value_losses).sum()\n",
    "\n",
    "    # perform backprop\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # reset rewards and action buffer\n",
    "    del model.rewards[:]\n",
    "    del model.saved_actions[:]\n",
    "\n",
    "try:\n",
    "    running_reward = 10\n",
    "\n",
    "    # run inifinitely many episodes\n",
    "    for i_episode in tqdm(range(500)):\n",
    "\n",
    "        # reset environment and episode reward\n",
    "        state = env.reset()\n",
    "        ep_reward = 0\n",
    "\n",
    "        # for each episode, only run 9999 steps so that we don't \n",
    "        # infinite loop while learning\n",
    "        for t in range(1, 1000):\n",
    "\n",
    "            # select action from policy\n",
    "            action = select_action(state , False)\n",
    "            # take the action\n",
    "            state, reward, done, _ = env.step([action])\n",
    "            if i_episode % 10 == 0:\n",
    "                env.render()\n",
    "            shaped_reward = reward + abs(state[0]+0.5) + state[1]*0.7\n",
    "            if state[0] >= 0.45:\n",
    "                shaped_reward += 200\n",
    "            model.rewards.append(shaped_reward)\n",
    "            ep_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        # update cumulative reward\n",
    "        running_reward = 0.05 * ep_reward + (1 - 0.05) * running_reward\n",
    "\n",
    "        # perform backprop\n",
    "        finish_episode()\n",
    "\n",
    "        # log results\n",
    "        if i_episode % 10 == 0:\n",
    "            print('Episode {}\\tLast reward: {:.2f}\\tAverage reward: {:.2f}'.format(\n",
    "                  i_episode, ep_reward, running_reward))\n",
    "\n",
    "        # check if we have \"solved\" the cart pole problem\n",
    "        if running_reward > env.spec.reward_threshold:\n",
    "            print(\"Solved! Running reward is now {} and \"\n",
    "                  \"the last episode runs to {} time steps!\".format(running_reward, t))\n",
    "            break\n",
    "finally:\n",
    "    env.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbf3537a8e7d4d24a8c1ecc4844bf0f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=500), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    for i_episode in tqdm(range(500)):\n",
    "\n",
    "        # reset environment and episode reward\n",
    "        state = env.reset()\n",
    "\n",
    "        for t in range(1, 1000):\n",
    "\n",
    "            # select action from policy\n",
    "            action = select_action(state , False)\n",
    "            # take the action\n",
    "            state, reward, done, _ = env.step([action])\n",
    "            if i_episode % 10 == 0:\n",
    "                env.render()\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "finally:\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
